{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGswBh_9SrqA",
        "outputId": "9a985041-8728-4ef3-edfd-0664afeeac28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_IaLKICV2_9",
        "outputId": "22685bea-38c2-4b00-b917-fd89cfcf0ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 203 images belonging to 2 classes.\n",
            "Found 50 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "7/7 [==============================] - 10s 1s/step - loss: 0.7044 - accuracy: 0.6010 - val_loss: 0.5598 - val_accuracy: 0.7200\n",
            "Epoch 2/15\n",
            "7/7 [==============================] - 8s 1s/step - loss: 0.5123 - accuracy: 0.7833 - val_loss: 0.5687 - val_accuracy: 0.7000\n",
            "Epoch 3/15\n",
            "7/7 [==============================] - 8s 1s/step - loss: 0.4458 - accuracy: 0.8177 - val_loss: 0.5033 - val_accuracy: 0.7400\n",
            "Epoch 4/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.4029 - accuracy: 0.8424 - val_loss: 0.5868 - val_accuracy: 0.7600\n",
            "Epoch 5/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.3390 - accuracy: 0.8522 - val_loss: 0.4838 - val_accuracy: 0.7800\n",
            "Epoch 6/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.2704 - accuracy: 0.8719 - val_loss: 0.5442 - val_accuracy: 0.8000\n",
            "Epoch 7/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.2061 - accuracy: 0.8966 - val_loss: 0.7680 - val_accuracy: 0.7400\n",
            "Epoch 8/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.2120 - accuracy: 0.9261 - val_loss: 0.3909 - val_accuracy: 0.8800\n",
            "Epoch 9/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.1222 - accuracy: 0.9704 - val_loss: 0.3802 - val_accuracy: 0.8400\n",
            "Epoch 10/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.0533 - accuracy: 0.9803 - val_loss: 0.6572 - val_accuracy: 0.8600\n",
            "Epoch 11/15\n",
            "7/7 [==============================] - 8s 1s/step - loss: 0.0682 - accuracy: 0.9754 - val_loss: 0.7350 - val_accuracy: 0.8400\n",
            "Epoch 12/15\n",
            "7/7 [==============================] - 8s 1s/step - loss: 0.0621 - accuracy: 0.9704 - val_loss: 0.4016 - val_accuracy: 0.8800\n",
            "Epoch 13/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.0359 - accuracy: 0.9901 - val_loss: 0.4756 - val_accuracy: 0.8800\n",
            "Epoch 14/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.5451 - val_accuracy: 0.9200\n",
            "Epoch 15/15\n",
            "7/7 [==============================] - 7s 1s/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 0.5276 - val_accuracy: 0.8600\n",
            "2/2 [==============================] - 1s 112ms/step - loss: 0.5276 - accuracy: 0.8600\n",
            "Validation accuracy: 0.8600000143051147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Define data directory (adjust the path if needed)\n",
        "data_dir = '/content/drive/MyDrive/Datasets/brain_tumor_dataset'  # Replace with the actual path\n",
        "\n",
        "# Create data generators for training and validation\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # 80% training, 20% validation\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary', # Assuming binary classification (tumor/no tumor)\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Binary crossentropy for binary classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,  # Adjust the number of epochs as needed\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print('Validation accuracy:', accuracy)\n",
        "\n",
        "\n",
        "# Save the model\n",
        "model.save('brain_tumor_detection_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDD3IrlibFk4"
      },
      "source": [
        "**ALZEIMIER DETECTION**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "alzheimer_data_dir = '/content/drive/MyDrive/Datasets/data'\n",
        "\n",
        "# Create data generators for Alzheimer's dataset\n",
        "alzheimer_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "alzheimer_train_generator = alzheimer_datagen.flow_from_directory(\n",
        "    alzheimer_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical', # Assuming multiple classes (e.g., mild, moderate, severe)\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "alzheimer_validation_generator = alzheimer_datagen.flow_from_directory(\n",
        "    alzheimer_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "\n",
        "# Build a new model for Alzheimer's (or modify the existing one)\n",
        "alzheimer_model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(len(alzheimer_train_generator.class_indices), activation='softmax') # Softmax for multi-class\n",
        "])\n",
        "\n",
        "# Compile the Alzheimer's model\n",
        "alzheimer_model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy', # Categorical crossentropy for multi-class\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the Alzheimer's model\n",
        "alzheimer_history = alzheimer_model.fit(\n",
        "    alzheimer_train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=alzheimer_validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the Alzheimer's model\n",
        "loss, accuracy = alzheimer_model.evaluate(alzheimer_validation_generator)\n",
        "print('Alzheimer\\'s Validation accuracy:', accuracy)\n",
        "\n",
        "# Save the Alzheimer's model\n",
        "alzheimer_model.save('alzheimer_detection_model.h5')"
      ],
      "metadata": {
        "id": "hgh1CSgMkGbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pneumonia Detection**"
      ],
      "metadata": {
        "id": "fmIFq1xThRxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Datasets/Viral Pneumonia'\n",
        "\n",
        "\n",
        "# Create data generators with data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # 80% for training, 20% for validation\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "\n",
        "# Build the CNN model (modified for better performance)\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(256, (3, 3), activation='relu'),  # Added another convolutional layer\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.5),  # Add dropout for regularization\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Callbacks for early stopping and model checkpointing\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('pneumonia_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,  # Increased epochs for better training\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping, model_checkpoint]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print('Validation accuracy:', accuracy)\n",
        "\n",
        "# Save the model (the best model is already saved due to ModelCheckpoint callback)\n",
        "model.save('pneumonia_detection_model.h5')"
      ],
      "metadata": {
        "id": "T9DffejJgt0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Covid Detection**"
      ],
      "metadata": {
        "id": "5Wd_M20BkXoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Define data directory (adjust the path if needed)\n",
        "data_dir = '/content/drive/MyDrive/Datasets/covid'  # Replace with the actual path\n",
        "\n",
        "# Create data generators for training and validation\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # 80% training, 20% validation\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary', # Assuming binary classification (tumor/no tumor)\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Binary crossentropy for binary classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,  # Adjust the number of epochs as needed\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print('Validation accuracy:', accuracy)\n",
        "\n",
        "\n",
        "# Save the model\n",
        "model.save('brain_tumor_detection_model.h5')\n"
      ],
      "metadata": {
        "id": "eaM9wl7_kUr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}